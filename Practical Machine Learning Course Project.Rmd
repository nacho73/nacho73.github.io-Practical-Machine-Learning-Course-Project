---
title: "Practical Machine Learning Coursera Project"
author: "José Ignacio Gavara"
date: "6/10/2020"
output: html_document
---

## Introduction  
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, we will to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.


## Load libraries
```{r, cache = T}
library(caret)
library(mlbench)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
```
### Download the Data
```{r, cache = T}
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "./data/pml-training.csv"
testFile  <- "./data/pml-testing.csv"
if (!file.exists("./data")) {
  dir.create("./data")
}
if (!file.exists(trainFile)) {
  download.file(trainUrl, destfile=trainFile, method="curl")
}
if (!file.exists(testFile)) {
  download.file(testUrl, destfile=testFile, method="curl")
}
```  
### Read the Data
  
```{r, cache = T}
trainRaw <- read.csv("./data/pml-training.csv")
testRaw <- read.csv("./data/pml-testing.csv")
dim(trainRaw)
dim(testRaw)
```

The training data set contains 19622 observations and 160 variables, while the testing data set contains 20 observations and 160 variables.

### Clean the data

```{r, cache = T}
sum(complete.cases(trainRaw))
```

First, we remove columns that contain NA missing values.
```{r, cache = T}
trainRaw <- trainRaw[, colSums(is.na(trainRaw)) == 0] 
testRaw <- testRaw[, colSums(is.na(testRaw)) == 0] 
```  
Next, we remove unncecessary columns.
```{r, cache = T}
classe <- trainRaw$classe
trainRemove <- grepl("^X|timestamp|window", names(trainRaw))
trainRaw <- trainRaw[, !trainRemove]
trainCleaned <- trainRaw[, sapply(trainRaw, is.numeric)]
trainCleaned$classe <- classe
testRemove <- grepl("^X|timestamp|window", names(testRaw))
testRaw <- testRaw[, !testRemove]
testCleaned <- testRaw[, sapply(testRaw, is.numeric)]
```

Now, the cleaned training data set contains 19622 observations and 53 variables, while the testing data set contains 20 observations and 53 variables. 

## Setting the data in test data and training data
Now, we will divide the data into training data (70%) and test data (30%)

```{r, cache = T}
set.seed(19654) 
inTrain <- createDataPartition(trainCleaned$classe, p=0.70, list=F)
trainData <- trainCleaned[inTrain, ]
testData <- trainCleaned[-inTrain, ]
```

## Data Modeling
We test two models and select the best.
We test the Random Forest Model and Generalized Boosted Model. 

We will choose the best model using cross-validation, taking 5 different pairs of training data-test data.

```{r, cache = T}

controlRf <- trainControl(method="cv", 5)

modelRf <- train(classe ~ ., data=trainData, method="rf", trControl=controlRf, ntree=250)
predictRf <- predict(modelRf, testData)
```

We will not show the construction of modelGBM, as this would suggest that a large number of results will be displayed on the screen. We build the model with the following code:

modelGBM <- train (classe ~., Data = trainData, method = "gbm", trControl = controlRf)
predictGBM <- predict (modelGBM, testData)


```{r gbm, include=FALSE}
library(caret)
modelGBM <- train(classe ~ ., data=trainData, method="gbm", trControl=controlRf)
predictGBM <- predict(modelGBM, testData)
```


## Showing models accuracy

```{r, cache = T}
results <- resamples(list(GBM=modelGBM, RF=modelRf))
summary(results)
```

So, the best model is modelRf, with 99.05% accuracy and 0,95% out of sample error rate. The GBM model has an accuracy of 95,90% and an 4,10% of out of sample error rate. 

## Predicting for Test Data Set
Now, we apply the model to the testing data set, to answer the prediction quiz.

```{r, cache = T}
result <- predict(modelRf, testCleaned[, -length(names(testCleaned))])
result
```  

## Appendix: Figures

1. Boxplots of results
```{r, cache = T}
bwplot(results)
```


2. Dot plots of results
```{r, cache = T}
dotplot(results)
```


3. Decision Tree Visualization
```{r, cache = T}
treeModel <- rpart(classe ~ ., data=trainData, method="class")
prp(treeModel) 
```
